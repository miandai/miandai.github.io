---
layout: post
keywords: Transformer
description: Transformer
title: "Transformer 知识点 Q&A"
categories: [Transformer]
tags: [Transformer]
---

* content
{:toc}

## Input Embedding

- Embedding层包含哪些参数？

这部分参数包括输入和输出的 token embeddings 以及 positional encodings。token embeddings将 tokens 映射到高维空间，而 positional encodings 则为模型提供序列中每个元素的位置信息。在大型模型中，由于词汇表的大小和序列长度的增加，这部分参数量会占据相当的比例。当然位置编码不一定增加参数量，要看是固定函数生成还是训练得到的参数。



## Multi-Head Attention

- 自注意力都有哪些参数？

参数包括映射矩阵Wq、Wk和Wv和用于合并多头输出的线性层。

- q、k、v 的向量维度相同吗？

q(i)和k(i)都是dk维向量。投影矩阵Wq和Wk的维度为d×dk，而Wv的维度为d×dv。d代表每个词向量的大小。

由于我们正在计算查询和键向量之间的点积，这两个向量必须包含相同数量的元素（dq = dk）。在许多语言大模型中，我们通常会使用相同大小的值向量，以确保dq = dk = dv。然而，决定上下文向量大小的值向量v(i)中的元素数量可以是任意的。

- 注意力权重如何做归一化？

应用softmax函数，将未归一化的注意力权重ω归一化，得到标准化的注意力权重α（alpha）。在通过softmax函数进行归一化之前，还需要使用1/√{dk}来对ω进行缩放。

通过dk的缩放可确保权重向量的欧几里德长度大致相同。这有助于防止注意力权重过小或过大，否则可能导致数值不稳定，甚至会影响模型在训练过程中的收敛能力。

- 在实践中，如果我们可以调节SelfAttention类中的输出嵌入大小，为什么还需要多个注意力头呢？

增加单个自注意力头的输出维度和使用多个注意力头的区别在于模型处理数据以及从数据中学习的方式。虽然这两种方法都增加了模型表示数据的不同特征或方面的能力，但它们之间有着本质上的区别。

例如，多头注意力中的每个注意力头都可以学习关注输入序列的不同部分，捕捉数据内部的各方面或关系。这种表示的多样性对于多头注意力的成功至关重要。
多头注意力可以更加高效，尤其是在并行计算方面。每个头都可以独立处理，这使其非常适合如今的硬件加速器，如擅长并行处理的GPU或TPU。
简而言之，使用多个注意头不仅能增加模型容量，还能增强其学习数据内部多样特征和关系的能力。例如，7B LLaMA 2模型使用了32个注意头。

## Cross Attention

- Transformer 的交叉注意力是如何实现的？

q 通常来自解码器，k、v 通常来自编码器。注意：计算交叉注意力时，解码器序列和编码器序列的数量不必相同。


## Masked Multi-Head Attention

- 在 self-attention 中，是怎么保证某个位置的输出仅基于先前位置的已知输出，而不基于未来位置？

因果自注意力机制通常也被称为“掩码自注意力（masked self-attention）”。在原始Transformer架构中，它对应“掩码多头注意力”模块。

为了在类似GPT的LLM中实现这一点，对于每个处理的词元，我们会掩码处理后续的词元，这些词元在输入文本中出现在当前词元之后。

两种掩码方法：

1. 首先计算注意力得分，然后计算注意力权重，掩码处理对角线以上的注意力权重，最后重新归一化注意力权重。

2. 一种更有效的方法可以达到相同结果。在这种方法中，我们得到注意力得分，并在将值输入softmax函数以计算注意力权重前，将对角线以上的值替换为负无穷。

## 参考资料：

1. 从头理解与编码LLM的自注意力机制：https://mp.weixin.qq.com/s/KTjStVcLIburEj4vaRjIuA
2. https://kimi.moonshot.cn/
3. https://www.baichuan-ai.com/chat
